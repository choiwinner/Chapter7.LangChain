{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c400862-e42c-4e3b-ab22-9c846a55d914",
   "metadata": {},
   "source": [
    "## Model I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf69c55-0e81-4a23-9f75-a44c62cbc4f4",
   "metadata": {},
   "source": [
    "### Message\n",
    "\n",
    "기본 메시지 인터페이스는 BaseMessage에 의해 정의되며, 두 가지 필수 속성이 있습니다\n",
    "\n",
    "- content: 메시지의 내용입니다. 보통 문자열입니다.\n",
    "- role: BaseMessage가 오는 엔티티입니다.\n",
    "\n",
    "\n",
    "LangChain은 다양한 역할을 쉽게 구분할 수 있는 여러 객체를 제공합니다:\n",
    "\n",
    "- HumanMessage: 사용자/인간으로부터 오는 BaseMessage입니다.\n",
    "- AIMessage: AI/어시스턴트로부터 오는 BaseMessage입니다.\n",
    "- SystemMessage: 시스템으로부터 오는 BaseMessage입니다.\n",
    "- 이들 역할 중 어느 것도 적절하지 않다면, `role`을 수동으로 지정할 수 있는 ChatMessage 클래스도 있습니다.\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [HumanMessage(content=text)]\n",
    "\n",
    "chat_model.invoke(messages)\n",
    "# >> AIMessage(content=\"Socks O'Color\")\n",
    "```\n",
    "\n",
    "ChatModel.invoke를 수행하면 `Message의 리스트`를 입력으로 받고 `Message`를 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa2acd38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "#os_key Setting\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env 파일 로드(api key load) \n",
    "#GOOGLE_API_KEY \n",
    "#LANGCHAIN_API_KEY \n",
    "#TAVILY_API_KEY \n",
    "#HUGGINGFACEHUB_API_TOKEN \n",
    "#COHERE_API_KEY \n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09fbba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOGLE_API_KEY 환경 변수 값 가져오기\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# genai.configure로 GOOGLE API KEY Setting하기기\n",
    "genai.configure(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b10a71b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN AI 예시시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e0f11f-3b4d-4ccf-900c-918b1715fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c05010a5-f074-47b8-80c3-3a7f2cde6a40",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\py31012\\lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\py31012\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:578\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy)\n\u001b[0;32m    577\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[1;32m--> 578\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msync_specific)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\py31012\\lib\\site-packages\\openai\\_client.py:110\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    108\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918eacd6-8f26-4ee4-a438-3df878546491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 무엇을 도와드릴까요?', response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 11, 'total_tokens': 32}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-727e9251-84ec-46b2-ae19-6464315a2cc5-0', usage_metadata={'input_tokens': 11, 'output_tokens': 21, 'total_tokens': 32})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"안녕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "067a640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55e88b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d8e9974",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ec417b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 무엇을 도와드릴까요?\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-352e1c46-7014-4525-af0c-f93f137404fb-0', usage_metadata={'input_tokens': 4, 'output_tokens': 15, 'total_tokens': 19})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#langchain_chatmodel_llm 생성 후 llm.invoke('text') 형태로만 입력해도 HumanMessage(content='text')형태로 입력된다.\n",
    "llm.invoke('안녕하세요')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc436f5-f96e-4116-b732-44f99f06dc20",
   "metadata": {},
   "source": [
    "## Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b886a98-e907-40be-bf65-7786e4931e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5542e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [HumanMessage(content='안녕')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c9a52a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 무엇을 도와드릴까요?\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-e658a373-0088-4757-8fd2-d6dca2513c38-0', usage_metadata={'input_tokens': 3, 'output_tokens': 15, 'total_tokens': 18})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "712fc4b4-0ed4-4ba9-99ec-63f20bc90d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화 시작\n",
    "# messages에 리스트 형태로 SystemMeasage()-instruction이나 페스소나 부여, HumanMessage()-명령으로 구성하고\n",
    "# llm.invoke(messages)를 입력한다.\n",
    "messages = [\n",
    "    SystemMessage(content=\"너는 AI 번역 모델이다.\"),\n",
    "    HumanMessage(content=\"'안녕' 이 문장이 영어로 뭐야?\"),\n",
    "]\n",
    "\n",
    "# 챗 모델을 호출하여 응답을 받습니다.\n",
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5928c26b-b056-416f-92ae-6d0a75cb5491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"안녕\"은 여러 상황에 따라 다르게 번역될 수 있습니다. 가장 흔한 표현은 \"Hello\" 또는 \"Hi\"입니다. 하지만 상황에 따라 다음과 같은 다른 표현을 사용할 수도 있습니다.\\n\\n* **Good morning:** 아침 인사\\n* **Good afternoon:** 오후 인사\\n* **Good evening:** 저녁 인사\\n* **Hey:**  \"Hi\"보다 좀 더 캐주얼한 표현\\n* **What\\'s up?:**  \"잘 지내?\"라는 의미의 캐주얼한 인사\\n* **How are you?:**  \"어떻게 지내세요?\"라는 의미의 정중한 인사\\n\\n어떤 상황에서 사용하고 싶으신가요? 더 자세한 맥락을 알려주시면 더 정확한 번역을 제공해 드릴 수 있습니다.\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-810acfe4-c953-464b-a404-3d06accb9938-0', usage_metadata={'input_tokens': 24, 'output_tokens': 194, 'total_tokens': 218})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f490ef2-1a60-446d-97df-8d9c26a328a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"안녕\"은 여러 상황에 따라 다르게 번역될 수 있습니다. 가장 흔한 표현은 \"Hello\" 또는 \"Hi\"입니다. 하지만 상황에 따라 다음과 같은 다른 표현을 사용할 수도 있습니다.\\n\\n* **Good morning:** 아침 인사\\n* **Good afternoon:** 오후 인사\\n* **Good evening:** 저녁 인사\\n* **Hey:**  \"Hi\"보다 좀 더 캐주얼한 표현\\n* **What\\'s up?:**  \"잘 지내?\"라는 의미의 캐주얼한 인사\\n* **How are you?:**  \"어떻게 지내세요?\"라는 의미의 정중한 인사\\n\\n어떤 상황에서 사용하고 싶으신가요? 더 자세한 맥락을 알려주시면 더 정확한 번역을 제공해 드릴 수 있습니다.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aa52e0-99c3-4d92-b175-6ce06f95c5bd",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f4db5c-d360-4121-850d-e8211805032c",
   "metadata": {},
   "source": [
    "Prompt template과 LLM 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcc0d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#langchain_core 내 ChatpromptTemplate와 휴먼,SystemMessagePromptTemplate를 만든다.\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ea3c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#system_prompt 정의하기\n",
    "system_prompt_template = \"당신은 {language} 언어 전문가다.\"\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(system_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2717feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#human_prompt 정의하기기\n",
    "human_message_prompt_template ='{text} 여기에서 키워드를 뽑아서 콤마로 구분해줘'\n",
    "human_promt = HumanMessagePromptTemplate.from_template(human_message_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4caf793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChatPromptTemplate.from_messages의 list로 system_prmpt와 human_promt 정의하기기\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([system_prompt,human_promt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9ef578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#완성된 chat_promt_template와 llm을 chain으로 연결하기\n",
    "\n",
    "chain = chat_prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af641f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain.invoke({dict})형태로 실행하기\n",
    "text_str = \"LangChain is a framework for developing applications powered by language models.\"\n",
    "output = chain.invoke({'text':text_str,'language':'영어'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d34bdb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain, language models, framework, applications\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d75e3e5e-b8e5-46bb-b184-e3bbbeed85dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea047ece-4436-4f1e-9d1b-927b6f7fa4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약 template\n",
    "human_message_prompt = \"'{text}' 여기서 키워드를 뽑아서 콤마로 구분해줘\"\n",
    "human_message_prompt_template = HumanMessagePromptTemplate.from_template(human_message_prompt)\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt_template])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56a897e4-17f0-4a69-a552-a9f4ff9518e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4267d4ba-0447-4919-a1f5-30cb301c0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = chain.invoke({\"text\": \"LangChain is a framework for developing applications powered by language models.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16281632-8245-492b-8a6a-c9d6afe54eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain, framework, language models, applications\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-1bede798-0cbe-4f5a-ae19-6fa3b182ade8-0', usage_metadata={'input_tokens': 34, 'output_tokens': 10, 'total_tokens': 44})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d994c502-974e-4126-ab8e-d3b91146254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain, framework, language models, applications\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py31012",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
